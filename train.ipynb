{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from utils import LABELS\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(landmarks):\n",
    "    processed_landmarks = []\n",
    "    # set origin to 0\n",
    "    # expand it\n",
    "    min_x = float('inf')\n",
    "    min_y = float('inf')\n",
    "    min_z = float('inf')\n",
    "    max_x = 0\n",
    "    max_y = 0\n",
    "    max_z = 0\n",
    "    for point in landmarks:\n",
    "        x = point[0]\n",
    "        y = point[1]\n",
    "        z = point[2]\n",
    "        min_x = min(min_x, x)\n",
    "        min_y = min(min_y, y)\n",
    "        min_z = min(min_z, z)\n",
    "        max_x = max(max_x, x)\n",
    "        max_y = max(max_y, y)\n",
    "        max_z = max(max_z, z)\n",
    "\n",
    "    offset_x = -min_x\n",
    "    offset_y = -min_y\n",
    "    offset_z = -min_z\n",
    "    scaler_x = 1 / (max_x + offset_x)\n",
    "    scaler_y = 1 / (max_y + offset_y)\n",
    "    scaler_z = 1 / (max_z + offset_z)\n",
    "    scaler = min(scaler_x, scaler_y)\n",
    "\n",
    "    for point in landmarks:\n",
    "        x = (point[0] + offset_x) * scaler\n",
    "        y = (point[1] + offset_y) * scaler\n",
    "        z = (point[2] + offset_z) * scaler_z\n",
    "        processed_landmarks.append([x, y, z])\n",
    "\n",
    "    return processed_landmarks    \n",
    "\n",
    "def split_train_test(x, y, test_size=0.3):\n",
    "    # shuffle data\n",
    "    temp = list(zip(x, y))\n",
    "    random.shuffle(temp)\n",
    "    x, y = zip(*temp)\n",
    "    x, y = list(x), list(y)\n",
    "    \n",
    "    # split data\n",
    "    cutoff_index = int(len(x) * (1 - test_size))\n",
    "    x_train = x[:cutoff_index]\n",
    "    x_test = x[cutoff_index:]\n",
    "    y_train = y[:cutoff_index]\n",
    "    y_test = y[cutoff_index:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data length: 1600\n",
      "Train data length: 1120\n",
      "Test data length: 480\n"
     ]
    }
   ],
   "source": [
    "data_root = os.path.abspath('raw_data')\n",
    "all_paths = list(tf.io.gfile.glob(data_root + r'/*/*'))\n",
    "if not all_paths:\n",
    "    raise ValueError('Image dataset directory is empty.')\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for path in all_paths:\n",
    "    label = os.path.basename(os.path.dirname(path))\n",
    "    hand_data = pickle.load(open(path, 'rb'))\n",
    "    for frame in hand_data:\n",
    "        processed_data = preprocess_data(frame.hand)\n",
    "        x.append(processed_data)\n",
    "        y.append(LABELS[label])\n",
    "\n",
    "assert len(x) == len(y)\n",
    "print(f\"Total data length: {len(x)}\")\n",
    "\n",
    "_x_train, _y_train, _x_test, _y_test = split_train_test(x, y)\n",
    "print(f\"Train data length: {len(_x_train)}\")\n",
    "print(f\"Test data length: {len(_x_test)}\")\n",
    "\n",
    "x_train = np.array(_x_train)\n",
    "y_train = np.array(_y_train)\n",
    "x_test = np.array(_x_test)\n",
    "y_test = np.array(_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(21, 3)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(8)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "35/35 [==============================] - 0s 541us/step - loss: 1.7548 - accuracy: 0.4946\n",
      "Epoch 2/5\n",
      "35/35 [==============================] - 0s 650us/step - loss: 1.1089 - accuracy: 0.9116\n",
      "Epoch 3/5\n",
      "35/35 [==============================] - 0s 572us/step - loss: 0.6731 - accuracy: 0.9804\n",
      "Epoch 4/5\n",
      "35/35 [==============================] - 0s 515us/step - loss: 0.4038 - accuracy: 0.9964\n",
      "Epoch 5/5\n",
      "35/35 [==============================] - 0s 528us/step - loss: 0.2506 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x15ee1bbd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 - 0s - loss: 0.1640 - accuracy: 1.0000 - 62ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16401298344135284, 1.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "probability_model.save('gesture_recognizer.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "predictions = probability_model(np.array([x_test[0]])).numpy()[0]   #type: ignore\n",
    "prediction = np.argmax(predictions)\n",
    "probability = predictions[prediction]\n",
    "print(prediction)\n",
    "print(y_test[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
