{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To be used in Google Colab\n",
        "\n"
      ],
      "metadata": {
        "id": "LpKD6NHpjPWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites\n",
        "Install the MediaPipe Model Maker package."
      ],
      "metadata": {
        "id": "sKRp_bAh5Usx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7pkqkn55QTA"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries."
      ],
      "metadata": {
        "id": "o7VZw3SH5rug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "k729CcaH5zNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the dataset\n",
        "\n",
        "The dataset for gesture recognition in model maker requires the following format: `<dataset_path>/<label_name>/<img_name>.*`. In addition, one of the label names (`label_names`) must be `none`. The `none` label represents any gesture that isn't classified as one of the other gestures."
      ],
      "metadata": {
        "id": "WSoFVlqr52ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/rps_data_sample.zip\n",
        "# !unzip rps_data_sample.zip\n",
        "dataset_path = \"/content/drive/MyDrive/Colab Data/drone_gesture_data\""
      ],
      "metadata": {
        "id": "Lbb01r4I7Wft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify the rock paper scissors dataset by printing the labels. There should be 4 gesture labels, with one of them being the `none` gesture."
      ],
      "metadata": {
        "id": "X_DrdZ809J9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_path)\n",
        "labels = []\n",
        "for i in os.listdir(dataset_path):\n",
        "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
        "    labels.append(i)\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "C0DmBzCg9LDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better understand the dataset, plot a couple of example images for each gesture."
      ],
      "metadata": {
        "id": "fZBbmWCu9UTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EXAMPLES = 5\n",
        "\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(dataset_path, label)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
        "    axs[i].get_xaxis().set_visible(False)\n",
        "    axs[i].get_yaxis().set_visible(False)\n",
        "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XO5SWrLw9XRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run\n",
        "The workflow consists of 4 steps which have been separated into their own code blocks.\n",
        "## Load the dataset\n",
        "Load the dataset located at `dataset_path` by using the `Dataset.from_folder` method. When loading the dataset, run the pre-packaged hand detection model from MediaPipe Hands to detect the hand landmarks from the images. Any images without detected hands are ommitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, rather than images themselves.\n",
        "\n",
        "The `HandDataPreprocessingParams` class contains two configurable options for the data loading process:\n",
        "* `shuffle`: A boolean controlling whether to shuffle the dataset. Defaults to true.\n",
        "* `min_detection_confidence`: A float between 0 and 1 controlling the confidence threshold for hand detection.\n",
        "\n",
        "Split the dataset: 80% for training, 10% for validation, and 10% for testing."
      ],
      "metadata": {
        "id": "MIUV3eNp-GJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=dataset_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ],
      "metadata": {
        "id": "x7WKH9q1AucZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "Train the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the Hyperparameters section below."
      ],
      "metadata": {
        "id": "ZglFqKBSA4-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ],
      "metadata": {
        "id": "f3Wv28KFBFDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model performance\n",
        "\n",
        "After training the model, evaluate it on a test dataset and print the loss and accuracy metrics."
      ],
      "metadata": {
        "id": "dKjlQcvVBShj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ],
      "metadata": {
        "id": "wy8phmzhBWf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export to Tensorflow Lite Model\n",
        "\n",
        "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file."
      ],
      "metadata": {
        "id": "1wWmY8hlBnTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.export_model()\n",
        "!ls exported_model"
      ],
      "metadata": {
        "id": "vVR4NM47BtqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('exported_model/gesture_recognizer.task')"
      ],
      "metadata": {
        "id": "_N3k9g-nB3q8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}